# AI Search and Optimization Project

A comprehensive, production-ready Python framework for comparing **Firefly Algorithm** (FA) with classical optimization methods on continuous and discrete benchmark problems.

## üéØ Project Overview

This project implements and benchmarks multiple optimization algorithms with:

- ‚úÖ **Full type hints** for all functions and classes
- ‚úÖ **Comprehensive error handling** with actionable error messages
- ‚úÖ **>80% test coverage** with edge case testing
- ‚úÖ **Parallel execution** support for faster benchmarking
- ‚úÖ **Academic-grade visualizations** following metaheuristic best practices
- ‚úÖ **Reproducible results** with fixed seeds
- ‚úÖ **Statistical analysis** with Wilcoxon and Friedman tests

### Algorithms Implemented

#### Swarm Intelligence
- **Firefly Algorithm (FA)** - Bio-inspired optimization
  - Continuous optimization variant
  - Discrete Knapsack variant with repair strategies

#### Classical Baselines
- **Hill Climbing (HC)** - Greedy local search with restart
- **Simulated Annealing (SA)** - Probabilistic local search with temperature scheduling
- **Genetic Algorithm (GA)** - Evolutionary optimization with elitism

### Benchmark Problems

#### Continuous Functions
- **Rastrigin** - Highly multimodal with many local minima
  - Dimensions: d=10, 30, 50
  - Global optimum: f(0,...,0) = 0
  - Domain: [-5.12, 5.12]^d
  - Three test configurations: quick convergence, multimodal escape, scalability

#### Discrete Problems
- **0/1 Knapsack** - Maximize value within capacity constraint
  - Sizes: n=50, 100, 200 items
  - 4 instance types: uncorrelated, weakly correlated, strongly correlated, subset-sum
  - DP optimal solution available for n ‚â§ 100
  - Multiple random seeds for statistical robustness

## üìÅ Project Structure

```
CSTTNT_DA1/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/                      # Base classes and utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_optimizer.py      # Abstract optimizer interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ problem_base.py        # Abstract problem interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Helper functions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ problems/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ continuous/            # Continuous benchmark functions
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rastrigin.py       # Rastrigin function
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ discrete/              # Discrete optimization problems
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ knapsack.py        # 0/1 Knapsack problem with DP solver
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ swarm/                     # Swarm intelligence algorithms
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fa.py                  # Firefly Algorithm (continuous & discrete)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ classical/                 # Classical baseline algorithms
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hill_climbing.py       # Hill Climbing with random restart
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simulated_annealing.py # Simulated Annealing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ genetic_algorithm.py   # Genetic Algorithm
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                     # Utility modules
‚îÇ       ‚îî‚îÄ‚îÄ visualization.py       # Academic-grade plotting functions
‚îÇ
‚îú‚îÄ‚îÄ test/                          # Unit tests (>80% coverage)
‚îÇ   ‚îú‚îÄ‚îÄ test_problems.py           # Tests for Rastrigin and Knapsack
‚îÇ   ‚îú‚îÄ‚îÄ test_firefly_algorithm.py  # Tests for Firefly Algorithm
‚îÇ   ‚îú‚îÄ‚îÄ test_classical_algorithms.py
‚îÇ   ‚îú‚îÄ‚îÄ test_edge_cases.py
‚îÇ   ‚îú‚îÄ‚îÄ test_parallel_execution.py
‚îÇ   ‚îú‚îÄ‚îÄ run_all_tests.py
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ benchmark/                     # Comprehensive benchmark suite
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Centralized benchmark configurations
‚îÇ   ‚îú‚îÄ‚îÄ instance_generator.py      # Knapsack instance generation
‚îÇ   ‚îú‚îÄ‚îÄ run_rastrigin.py          # Rastrigin benchmark runner
‚îÇ   ‚îú‚îÄ‚îÄ run_knapsack.py           # Knapsack benchmark runner
‚îÇ   ‚îú‚îÄ‚îÄ analyze_results.py        # Statistical analysis (Wilcoxon, Friedman)
‚îÇ   ‚îú‚îÄ‚îÄ visualize.py              # Generate all plots
‚îÇ   ‚îú‚îÄ‚îÄ run_all.py                # Master script (parallel execution)
‚îÇ   ‚îú‚îÄ‚îÄ run_all.sh                # Shell script wrapper
‚îÇ   ‚îú‚îÄ‚îÄ test_benchmarks.py        # Benchmark integration tests
‚îÇ   ‚îú‚îÄ‚îÄ results/                  # Auto-generated results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rastrigin/           # Rastrigin results by config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ knapsack/            # Knapsack results by instance
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plots/               # All visualizations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logs/                # Execution logs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summaries/           # Statistical summaries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rastrigin_summary.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ knapsack_summary.csv
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Detailed benchmark documentation
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Legacy results directory (deprecated)
‚îú‚îÄ‚îÄ demo.py                        # Quick demonstration script
‚îú‚îÄ‚îÄ environment.yml                # Conda environment
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ README.md                      # This file
‚îî‚îÄ‚îÄ QUICKSTART.md                  # Quick start guide
```

## üöÄ Getting Started

### Prerequisites

- Python 3.10+
- NumPy
- SciPy (for statistical tests)
- Matplotlib (for visualization)
- Pandas (for data analysis)
- pytest (for testing)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/1234quan1234/CSTTNT_DA1.git
cd CSTTNT_DA1
```

2. Install dependencies using conda (recommended):
```bash
conda env create -f environment.yml
conda activate aisearch
```

Or using pip:
```bash
pip install -r requirements.txt
```

### Quick Start

#### Option 1: Run Complete Benchmark Suite (Recommended)

```bash
# Fast mode with parallel execution (use all CPU cores)
python benchmark/run_all.py --quick --jobs -1

# Full benchmark with 4 parallel workers
python benchmark/run_all.py --full --jobs 4
```

This will:
- Run all Rastrigin configurations (quick_convergence, multimodal_escape, scalability)
- Run all Knapsack instances (n=50, 100, 200 with 4 instance types)
- Perform 30 independent runs per configuration
- Generate statistical analysis (mean, std, median, Wilcoxon tests, Friedman tests)
- Create all visualizations in `benchmark/results/plots/`

#### Option 2: Run Individual Benchmarks

**Rastrigin Benchmark:**
```bash
# Quick convergence test (d=10, ~2 minutes with 4 cores)
python benchmark/run_rastrigin.py --config quick_convergence --jobs 4

# Multimodal escape test (d=30, ~5 minutes)
python benchmark/run_rastrigin.py --config multimodal_escape --jobs -1

# Scalability test (d=50, ~10 minutes)
python benchmark/run_rastrigin.py --config scalability --jobs -1
```

**Knapsack Benchmark:**
```bash
# Small instances (n=50, ~5 minutes with 4 cores)
python benchmark/run_knapsack.py --size 50 --jobs 4

# Medium instances with DP optimal (n=100, ~15 minutes)
python benchmark/run_knapsack.py --size 100 --jobs -1

# Large instances (n=200, ~30 minutes)
python benchmark/run_knapsack.py --size 200 --jobs 4
```

#### Option 3: Analysis and Visualization Only

If you already have benchmark results:

```bash
# Generate statistical analysis
python benchmark/analyze_results.py --problem all

# Generate all plots
python benchmark/visualize.py
```

### Quick Test

Test individual algorithm implementations:

```bash
# Test Firefly Algorithm
python src/swarm/fa.py

# Test problem definitions
python src/problems/continuous/rastrigin.py
python src/problems/discrete/knapsack.py

# Test classical algorithms
python src/classical/hill_climbing.py
python src/classical/simulated_annealing.py
python src/classical/genetic_algorithm.py
```

## üí° Usage Examples

### Example 1: Rastrigin Function with Firefly Algorithm

```python
import numpy as np
from src.problems.continuous.rastrigin import RastriginProblem
from src.swarm.fa import FireflyContinuousOptimizer

# Create problem instance
problem = RastriginProblem(dim=10)

# Create optimizer with parameters tuned for multimodal problems
optimizer = FireflyContinuousOptimizer(
    problem=problem,
    n_fireflies=40,
    alpha=0.3,      # Higher randomization for exploration
    beta0=1.0,      # Base attractiveness
    gamma=1.0,      # Light absorption coefficient
    seed=42         # For reproducibility
)

# Run optimization
best_solution, best_fitness, history, trajectory = optimizer.run(max_iter=100)

print(f"Best fitness: {best_fitness:.6f}")
print(f"Error to optimum: {abs(best_fitness):.6f}")
print(f"Convergence: {history[0]:.4f} -> {history[-1]:.4f}")
```

### Example 2: Knapsack with Discrete Firefly Algorithm

```python
import numpy as np
from src.problems.discrete.knapsack import KnapsackProblem
from src.swarm.fa import FireflyKnapsackOptimizer

# Create Knapsack instance (uncorrelated type)
rng = np.random.RandomState(42)
n_items = 50
values = rng.randint(10, 100, n_items)
weights = rng.randint(1, 50, n_items)
capacity = int(0.5 * np.sum(weights))

problem = KnapsackProblem(values, weights, capacity)

# Compute DP optimal for comparison (feasible for n ‚â§ 100)
dp_optimal = problem.solve_dp()
print(f"DP Optimal: {dp_optimal}")

# Create optimizer with REPAIR strategy (fair comparison)
optimizer = FireflyKnapsackOptimizer(
    problem=problem,
    n_fireflies=60,
    alpha_flip=0.2,
    max_flips_per_move=3,
    constraint_handling="repair",  # NEW: 'repair' or 'penalty'
    seed=42
)

# Run optimization
best_sol, best_fit, history, trajectory = optimizer.run(max_iter=166)

total_value = -best_fit  # Negate for actual value
total_weight = np.sum(best_sol * weights)
optimality_gap = (dp_optimal - total_value) / dp_optimal * 100

print(f"Best value: {total_value:.0f} (DP: {dp_optimal:.0f})")
print(f"Optimality gap: {optimality_gap:.2f}%")
print(f"Weight: {total_weight:.0f}/{capacity} ({total_weight/capacity*100:.1f}%)")
print(f"Items selected: {np.sum(best_sol)}/{n_items}")
```

**Constraint Handling Strategies:**

The framework supports two strategies for handling Knapsack capacity constraints:

1. **Repair Strategy** (`constraint_handling="repair"`):
   - Infeasible solutions are repaired using greedy removal
   - Ensures all solutions are feasible
   - Fair comparison across all algorithms
   - Recommended for benchmarking

2. **Penalty Strategy** (`constraint_handling="penalty"`):
   - Infeasible solutions receive large penalty
   - Allows exploration of infeasible space
   - May find better solutions by "cutting corners"
   - Original implementation behavior

**Run benchmarks with both strategies:**
```bash
# Repair strategy only (fair comparison)
python benchmark/run_knapsack.py --size 50 --constraint repair

# Penalty strategy only (original behavior)
python benchmark/run_knapsack.py --size 50 --constraint penalty

# Both strategies for comparison
python benchmark/run_knapsack.py --size 50 --constraint both
```

### Example 3: Compare All Algorithms

```python
from src.problems.continuous.rastrigin import RastriginProblem
from src.swarm.fa import FireflyContinuousOptimizer
from src.classical.simulated_annealing import SimulatedAnnealingOptimizer
from src.classical.hill_climbing import HillClimbingOptimizer
from src.classical.genetic_algorithm import GeneticAlgorithmOptimizer

problem = RastriginProblem(dim=10)
max_iter = 125  # Same budget for fair comparison

# Firefly Algorithm
fa = FireflyContinuousOptimizer(problem, n_fireflies=40, alpha=0.3, seed=42)
_, fa_fit, fa_hist, _ = fa.run(max_iter=max_iter)

# Simulated Annealing
sa = SimulatedAnnealingOptimizer(problem, initial_temp=100, cooling_rate=0.95, seed=42)
_, sa_fit, sa_hist, _ = sa.run(max_iter=max_iter)

# Hill Climbing with restart
hc = HillClimbingOptimizer(problem, num_neighbors=20, restart_interval=50, seed=42)
_, hc_fit, hc_hist, _ = hc.run(max_iter=max_iter)

# Genetic Algorithm
ga = GeneticAlgorithmOptimizer(problem, pop_size=40, crossover_rate=0.8, seed=42)
_, ga_fit, ga_hist, _ = ga.run(max_iter=max_iter)

print(f"FA: {abs(fa_fit):.6f}")
print(f"SA: {abs(sa_fit):.6f}")
print(f"HC: {abs(hc_fit):.6f}")
print(f"GA: {abs(ga_fit):.6f}")
```

## üìä Benchmark Configurations

### Rastrigin Configurations

| Config Name | Dimension | Budget (evals) | Max Iter | Success Thresholds | Purpose |
|-------------|-----------|----------------|----------|--------------------|---------|
| `quick_convergence` | 10 | 10,000 | 250 | Gold: 1.0, Silver: 5.0, Bronze: 10.0 | Fast convergence test |
| `multimodal_escape` | 30 | 30,000 | 500 | Gold: 1.0, Silver: 10.0, Bronze: 50.0 | Escape local minima |
| `scalability` | 50 | 50,000 | 625 | Gold: 10.0, Silver: 50.0, Bronze: 100.0 | High-dimensional scaling |

**Success Levels:**
- **Gold** ü•á: Very close to global optimum (toughest threshold)
- **Silver** ü•à: Escaped bad regions, found good local minimum
- **Bronze** ü•â: Escaped worst regions (most lenient threshold)

**Algorithm Parameters:**
- **FA**: n_fireflies=40, Œ±=0.3, Œ≤‚ÇÄ=1.0, Œ≥=1.0
- **SA**: T‚ÇÄ=100, cooling=0.95, step=0.5
- **HC**: neighbors=20, step=0.5, restart=50
- **GA**: pop=40, crossover=0.8, mutation=0.1

### Knapsack Configurations

| n Items | Instance Types | Seeds | Budget (evals) | Max Iter (FA/GA) | Max Iter (SA/HC) | DP Optimal? |
|---------|----------------|-------|----------------|------------------|------------------|-------------|
| 50 | All 4 types | 42, 123, 999 | 10,000 | 166 | 10,000 | ‚úì Yes |
| 100 | All 4 types | 42, 123, 999 | 15,000 | 250 | 15,000 | ‚úì Yes |
| 200 | Uncorr, Weak | 42, 123, 999 | 30,000 | 500 | 30,000 | ‚úó No |

**Instance Types:**
1. **Uncorrelated**: Random values and weights
2. **Weakly Correlated**: values ‚âà weights ¬± noise
3. **Strongly Correlated**: values = weights + 100
4. **Subset-Sum**: values = weights (hardest)

**Algorithm Parameters:**
- **FA**: n_fireflies=60, Œ±_flip=0.2, max_flips=3, repair="greedy_remove"
- **SA**: T‚ÇÄ=1000, cooling=0.95
- **HC**: neighbors=20, restart=100
- **GA**: pop=60, crossover=0.8, mutation=1/n, elitism=0.1

## üìà Output Format

All benchmark results are saved in JSON format for reproducibility.

### Rastrigin Results

**File naming:** `benchmark/results/rastrigin/rastrigin_{config}_{algo}_{scenario}_{timestamp}.json`

```json
{
  "metadata": {
    "problem": "rastrigin",
    "config_name": "quick_convergence",
    "algorithm": "FA",
    "scenario": "out_of_the_box",
    "timestamp": "20251110T200402",
    "dimension": 10,
    "budget": 10000,
    "max_iter": 250,
    "pop_size": 40,
    "problem_seed": 42,
    "n_runs": 30,
    "n_successful": 28,
    "n_failed": 2,
    "status_breakdown": {
      "ok": 28,
      "timeout": 1,
      "nan": 1
    },
    "thresholds_used": {
      "gold": 1.0,
      "silver": 5.0,
      "bronze": 10.0
    },
    "avg_budget_utilization": 0.998
  },
  "all_results": [
    {
      "algorithm": "FA",
      "seed": 0,
      "algo_seed": 0,
      "problem_seed": 42,
      "best_fitness": 8.4567,
      "history": [45.6, 34.2, 23.1, 15.8, 8.4567],
      "elapsed_time": 2.15,
      "evaluations": 10000,
      "budget": 10000,
      "budget_utilization": 1.0,
      "success_levels": {
        "gold": {
          "success": false,
          "threshold": 1.0,
          "hit_evaluations": null
        },
        "silver": {
          "success": false,
          "threshold": 5.0,
          "hit_evaluations": null
        },
        "bronze": {
          "success": true,
          "threshold": 10.0,
          "hit_evaluations": 8200
        }
      },
      "status": "ok",
      "error_type": null,
      "error_msg": null
    }
  ]
}
```

**Key Improvements:**
- **Metadata**: Centralized configuration tracking, status breakdown, budget utilization metrics
- **Tracking**: Every run has `status`, `error_type`, `error_msg` for error investigation
- **Hit time**: `hit_evaluations` records when target threshold was achieved (null if never hit)
- **Budget control**: `budget_utilization` ensures algorithm stayed within evaluation budget
- **Problem seed**: `problem_seed` enables problem reproducibility

### Knapsack Results

**File naming:** `benchmark/results/knapsack/knapsack_n{size}_{type}_seed{seed}_{algo}_{timestamp}.json`

```json
{
  "metadata": {
    "problem": "knapsack",
    "n_items": 50,
    "instance_type": "uncorrelated",
    "instance_seed": 42,
    "algorithm": "FA",
    "timestamp": "20251110T202419",
    "capacity": 500.0,
    "budget": 10000,
    "n_runs": 30,
    "n_successful": 30,
    "n_failed": 0,
    "status_breakdown": {
      "ok": 30
    },
    "dp_optimal": 2450.0,
    "avg_gap_%": 2.34,
    "avg_feasibility_rate": 1.0
  },
  "all_results": [
    {
      "algorithm": "FA",
      "seed": 0,
      "algo_seed": 0,
      "best_value": 2387.0,
      "best_fitness": -2387.0,
      "total_weight": 487.5,
      "is_feasible": true,
      "history": [-1200.0, -1500.0, ..., -2387.0],
      "elapsed_time": 3.45,
      "items_selected": 18,
      "capacity_utilization": 0.975,
      "optimality_gap": 2.57,
      "status": "ok",
      "error_type": null,
      "error_msg": null
    },
    ...
  ]
}
```

### Summary CSV Files

Auto-generated by `analyze_results.py` with **NEW columns** for advanced analysis:

**rastrigin_summary.csv** (NEW columns marked with ‚≠ê):

```csv
Configuration,Algorithm,Mean,Std,Median,Best,Worst,Q1,Q3,Mean_Time,SR_<=0.1,SR_<=0.001,SR_<=1e-05,HT_med_<=0.1,HT_med_<=0.001,HT_med_<=1e-05,AUC_median,AUC_mean
quick_convergence,FA,8.45,2.31,7.89,3.21,15.67,6.12,10.23,2.15,0.83,0.45,0.10,245,892,2450,156.34,180.45
quick_convergence,GA,10.12,3.45,9.56,4.23,18.90,7.34,12.67,2.34,0.70,0.30,0.05,412,1234,3456,198.76,215.32
multimodal_escape,FA,45.67,12.34,43.21,28.90,78.45,36.78,54.32,6.89,0.60,0.25,0.08,867,3421,8934,456.78,512.34
...
```

**New columns explained:**
- ‚≠ê `SR_<=tol`: Success rate at tolerance (% runs achieving error ‚â§ tol)
- ‚≠ê `HT_med_<=tol`: Median hitting time (evaluations to reach tolerance)
- ‚≠ê `AUC_median`: Anytime performance (log integral, lower is better)
- ‚≠ê `AUC_mean`: Mean AUC across all runs

**knapsack_summary.csv** (NEW columns marked with ‚≠ê):

```csv
n_items,type,seed,Algorithm,Mean_Value,Std_Value,Mean_Gap_%,Std_Gap_%,Feasibility_Rate,Mean_Time,DP_Optimal,Mean_Norm_Value,Std_Norm_Value,SR_Gap_<=1.0%,SR_Gap_<=5.0%,SR_Gap_<=10.0%,HT_med_<=1%_gap
50,uncorrelated,42,FA,2387.5,45.6,2.34,1.12,1.00,3.45,2450.0,0.9744,0.0186,0.67,0.93,0.97,1250
50,uncorrelated,42,GA,2410.3,38.9,1.56,0.98,1.00,3.67,2450.0,0.9837,0.0159,0.87,0.97,1.00,892
50,uncorrelated,42,SA,2245.8,67.3,8.33,2.74,0.97,2.89,2450.0,0.9165,0.0275,0.10,0.53,0.80,3456
...
```

**New columns explained:**
- ‚≠ê `Mean_Norm_Value`: Normalized value (best_value / dp_optimal)
- ‚≠ê `SR_Gap_<=X%`: Success rate achieving gap ‚â§ X%
- ‚≠ê `HT_med_<=1%_gap`: Median hitting time to 1% gap target

### Global Ranks CSV Files

‚≠ê **NEW**: Generated by `generate_rastrigin_global_ranks()` and `generate_knapsack_global_ranks()`.

**rastrigin_global_ranks.csv:**
```csv
Algorithm,Avg_Rank,N_Configs
FA,1.47,3
GA,2.13,3
SA,2.87,3
HC,3.53,3
```

**knapsack_global_ranks.csv:**
```csv
Algorithm,Avg_Rank,N_Configs
GA,1.34,36
FA,1.89,36
SA,3.12,36
HC,4.65,36
```

These ranks show **average position across all configurations** ‚Äî crucial for fair comparison.

## üìä Visualizations (Academic Standards)

All plots follow metaheuristic benchmarking best practices and are saved in `benchmark/results/plots/`.

### Rastrigin Visualizations

1. **Convergence Curves** (`rastrigin_{config}_convergence.png`)
   - X-axis: Function evaluations (not iterations)
   - Y-axis: Error to optimum |f(x) - 0| (log scale)
   - Median trajectory with IQR (25-75%) shaded bands
   - Shows convergence speed fairly across algorithms

2. **Final Error Boxplots** (`rastrigin_{config}_boxplot.png`)
   - Distribution of final errors across 30 runs
   - Log scale for better visualization
   - Mean markers (red diamonds)
   - Shows robustness and outlier behavior

3. **ECDF Plots** (`rastrigin_{config}_ecdf.png`)
   - Empirical Cumulative Distribution Function
   - Shows P(error ‚â§ x) for each algorithm
   - Better than mean/median for tail behavior analysis

4. **Scalability Plot** (`rastrigin_scalability.png`)
   - Mean error vs dimension (d=10/30/50)
   - Log scale with error bars (¬±1 std)
   - Shows which algorithms scale well to higher dimensions

5. **‚≠ê NEW: Fixed-Target ECDF** (`rastrigin_ecdf_{config}.png`)
   - COCO/BBOB standard runtime-to-target visualization
   - Shows empirical CDF of evaluations needed to reach target
   - Separate plots for Gold/Silver/Bronze targets
   - Lines for algorithms, linestyles for scenarios

6. **‚≠ê NEW: Expected Running Time (ERT)** (`rastrigin_ert_{level}.png`)
   - Bar charts with confidence intervals
   - Expected evaluations to reach target (includes failed runs)
   - Grouped by configuration and scenario
   - Standard COCO benchmark metric

7. **‚≠ê NEW: Fixed-Budget Performance** (`rastrigin_fixed_budget_{config}.png`)
   - Anytime performance curves
   - Error achieved at different budget fractions (10%, 30%, 50%, 100%)
   - Shows convergence trajectory across optimization

8. **‚≠ê NEW: Performance Profiles** (`rastrigin_perf_profile.png`)
   - Dolan-Mor√© performance profiles
   - Fraction of problems solved within œÑ √ó best time
   - Standard for robustness comparison

9. **‚≠ê NEW: Data Profiles** (`rastrigin_data_profile.png`)
   - Mor√©-Wild data profiles
   - Fraction of problems solved as function of budget
   - Complements performance profiles

10. **‚≠ê NEW: Diversity Analysis** (`rastrigin_diversity_{config}.png`)
    - Population diversity metrics normalized by ‚àöD
    - Shows Initial, Mid-point (50%), Final, and Drop
    - Critical for diagnosing premature convergence

11. **‚≠ê NEW: Stagnation Analysis** (`rastrigin_stagnation_{config}.png`)
    - Histogram and ECDF of stagnation lengths
    - Longest period without improvement
    - Identifies search stalling

12. **‚≠ê NEW: Anytime AUC** (`rastrigin_anytime_auc_{config}.png`)
    - Area under convergence curve (lower is better)
    - Integrated performance measure
    - Shows overall convergence quality

### Knapsack Visualizations

**Per-Instance Plots:**

1. **Convergence Curves** (`knapsack_n{size}_{type}_seed{seed}_convergence.png`)
   - X-axis: Function evaluations
   - Y-axis: Best value found (higher is better)
   - Median with IQR bands
   - **DP optimal reference line** (red dashed) when available

2. **Optimality Gap Boxplots** (`knapsack_n{size}_{type}_seed{seed}_gap_boxplot.png`)
   - Distribution of (DP_opt - best_value) / DP_opt √ó 100%
   - Only for n=50, n=100 where DP is feasible
   - Lower is better (0% = optimal)

**Aggregate Plots:**

3. **Feasibility Rate** (`knapsack_feasibility.png`)
   - Bar chart showing % of feasible solutions
   - Grouped by n_items (50/100/200)
   - Sub-grouped by instance type
   - **Critical metric**: Algorithms violating constraints are penalized

4. **Capacity Utilization** (`knapsack_capacity_utilization.png`)
   - Boxplots of weight_used / capacity
   - Grouped by n_items
   - Green line at 1.0 = perfect utilization
   - Values >1.0 indicate constraint violations

5. **Runtime vs Quality** (`knapsack_runtime_quality.png`)
   - Scatter plot of elapsed_time vs optimality_gap
   - Shows Pareto front of fast-but-good algorithms
   - Color-coded by algorithm
   - Helps identify practical trade-offs

6. **Scalability Plots** (`knapsack_{type}_seed{seed}_scalability.png`)
   - Mean optimality gap vs n_items (50/100/200)
   - Error bars show ¬±1 std
   - Generated for uncorrelated and weakly_correlated types

## üìà Performance Metrics

### Rastrigin Metrics

| Metric | Description | Interpretation |
|--------|-------------|----------------|
| **Error to Optimum** | \|f(x) - 0\| | Lower is better (0 = perfect) |
| **Convergence Speed** | Evals to reach target | Faster is better |
| **Success Rate** | % runs achieving target error | Higher is better |
| **ECDF** | P(error ‚â§ x) | Shows distribution tail |

### Knapsack Metrics

| Metric | Description | Interpretation |
|--------|-------------|----------------|
| **Optimality Gap** | (DP_opt - value) / DP_opt √ó 100% | Lower is better (0% = optimal) |
| **Feasibility Rate** | % feasible solutions | **Must be 100%** |
| **Capacity Utilization** | weight_used / capacity | Higher is better (‚â§1.0) |
| **Runtime** | Elapsed time (seconds) | Lower is better |

### Statistical Tests

The benchmark suite performs rigorous statistical analysis:

1. **Friedman Test** (non-parametric ANOVA)
   - Tests if algorithms have significantly different performance
   - Reports average ranks (lower is better)
   - p-value < 0.05 indicates significant differences

2. **Wilcoxon Signed-Rank Test** (pairwise)
   - Compares each pair of algorithms
   - Reports p-values in matrix form
   - p-value < 0.05 indicates significant difference
   - Bonferroni correction for multiple comparisons

Example output:
```
Average Ranks (lower is better):
  FA: 1.47
  GA: 2.13
  SA: 2.87
  HC: 3.53

Pairwise Wilcoxon (p-values):
          FA      GA      SA      HC
FA        ‚Äî   0.0234  0.0001  0.0000
GA   0.0234       ‚Äî   0.0012  0.0000
SA   0.0001  0.0012       ‚Äî   0.0345
HC   0.0000  0.0000  0.0345       ‚Äî
```

## üéØ Expected Results

### Rastrigin Performance

| Algorithm | d=10 | d=30 | d=50 | Scaling | Strengths |
|-----------|------|------|------|---------|-----------|
| **FA** | ‚úì‚úì‚úì | ‚úì‚úì | ‚úì | Good | Fast early convergence, swarm cooperation |
| **GA** | ‚úì‚úì | ‚úì‚úì | ‚úì‚úì | Excellent | Stable across dimensions, genetic diversity |
| **SA** | ‚úì | ‚úó | ‚úó | Poor | Struggles with high-dimensional multimodal |
| **HC** | ‚úó | ‚úó | ‚úó | Poor | Gets trapped in local minima |

**Key Findings:**
- FA achieves best performance on d=10 due to effective swarm search
- GA maintains consistent quality across all dimensions
- SA and HC struggle with multimodal landscapes

### Knapsack Performance

| Algorithm | Uncorr | Weakly | Strongly | Subset | Strengths |
|-----------|--------|--------|----------|--------|-----------|
| **FA** | ‚úì‚úì | ‚úì‚úì | ‚úì‚úì‚úì | ‚úì | Good balance, effective repair |
| **GA** | ‚úì‚úì‚úì | ‚úì‚úì‚úì | ‚úì‚úì | ‚úì‚úì | Best overall, strong crossover |
| **SA** | ‚úì | ‚úì | ‚úì | ‚úó | Decent for easy instances |
| **HC** | ‚úó | ‚úó | ‚úó | ‚úó | Poor exploration |

**Key Findings:**
- FA/GA achieve <5% optimality gap for n‚â§100
- Strongly correlated instances favor swarm intelligence
- Subset-sum is hardest for all algorithms (exact value=weight matching)
- Repair strategies are critical for maintaining feasibility

## üß™ Testing

### Run Complete Test Suite

```bash
# Run all tests with coverage report
pytest test/ --cov=src --cov=benchmark --cov-report=html --cov-report=term

# View HTML coverage report
xdg-open htmlcov/index.html  # Linux
open htmlcov/index.html      # macOS
```

### Test Coverage Status

| Module | Coverage Target | Status |
|--------|-----------------|--------|
| `src/core/*.py` | 90%+ | ‚úì Achieved |
| `src/swarm/*.py` | 80%+ | ‚úì Achieved |
| `src/classical/*.py` | 80%+ | ‚úì Achieved |
| `src/problems/*.py` | 85%+ | ‚úì Achieved |
| `benchmark/*.py` | 70%+ | ‚úì Achieved |

### Test Categories

**Unit Tests (`test/`):**
- `test_continuous_problems.py` - Continuous benchmark functions
- `test_knapsack_problem.py` - Knapsack problem and DP solver
- `test_firefly_algorithm.py` - FA continuous and discrete variants
- `test_classical_algorithms.py` - HC, SA, GA implementations
- `test_edge_cases.py` - Boundary conditions, extreme inputs
- `test_parallel_execution.py` - Concurrency, reproducibility
- `test_utils.py` - Utility and visualization functions

**Integration Tests:**
```bash
# Quick integration test (5 runs per config)
python test/run_all_tests.py --quick

# Full integration test (30 runs per config)
python test/run_all_tests.py --full
```

**Benchmark Tests:**
```bash
# Test benchmark infrastructure
pytest benchmark/test_benchmarks.py -v
```

## üìö References

1. Yang, X. S. (2008). *Nature-inspired metaheuristic algorithms*. Luniver press.
2. Yang, X. S. (2010). "Firefly algorithm, stochastic test functions and design optimisation". *International Journal of Bio-Inspired Computation*, 2(2), 78-84.
3. [Rastrigin Function - Virtual Library of Simulation Experiments](https://www.sfu.ca/~ssurjano/rastr.html)
4. [Knapsack Problem - Wikipedia](https://en.wikipedia.org/wiki/Knapsack_problem)
5. Pisinger, D. (1995). "An expanding-core algorithm for the exact 0-1 knapsack problem". *European Journal of Operational Research*, 87(1), 175-187.
6. Wilcoxon, F. (1945). "Individual comparisons by ranking methods". *Biometrics Bulletin*, 1(6), 80-83.
7. Friedman, M. (1937). "The use of ranks to avoid the assumption of normality implicit in the analysis of variance". *Journal of the American Statistical Association*, 32(200), 675-701.

## üë• Contributors

- B√πi Anh Qu√¢n (@1234quan1234)

## üìù License

This project is for educational purposes as part of the CSTTNT (C∆° S·ªü Tr√≠ Tu·ªá Nh√¢n T·∫°o) course at HCMUS (Ho Chi Minh City University of Science).

---

**For detailed benchmark documentation, see:** [`benchmark/README.md`](benchmark/README.md)

**For quick start guide, see:** [`QUICKSTART.md`](QUICKSTART.md)

**For testing guide, see:** [`test/README.md`](test/README.md)